<center>lecture6 CNN架构</center>

# 批量归一化

该方法具有一定的重要性，

1. 输入$x$未以0未中心（需要大的偏置调整）
2. 输入$x$的不同元素缩放不一致。
   如果输入数据的不同元素的尺度差异很大，权重矩阵的各个元素需要相应的变化很大，会使得训练过程更加复杂和不稳定

通常插入在FC和tanh之间，在tanh之前。

![image-20240729205721041](D:\zjPhD\notes\notes\AI\cs231n\图片\25.png)

# 层归一化

* 归一化是在每个样本的特征维度上进行的，依赖于每个样本的数据。

* 训练和测试阶段处理相同。

![image-20240729205820673](D:\zjPhD\notes\notes\AI\cs231n\图片\26.png)

# 实例归一化

![image-20240729210150064](D:\zjPhD\notes\notes\AI\cs231n\图片\27.png)

# 不同归一化方法的对比

在卷积神经网络中，我们有很多规划化的方法，包括批量归一化（Batch Normalization）、层归一化（Layer Normalization）、实例归一化（Instance Normalization）和组归一化（Group Normalization）。

### 批量归一化（Batch Normalization）

- **归一化范围**：批量维度（N）和空间维度（H, W）。
- **应用**：对整个批次的数据进行归一化处理。
- **特点**：在训练时依赖当前批次的均值和方差，测试时使用训练时的累计均值和方差。

### 层归一化（Layer Normalization）

- **归一化范围**：特征维度（C）和空间维度（H, W）。
- **应用**：对每个样本的所有特征进行归一化处理。
- **特点**：在训练和测试期间行为一致，不受批次大小的影响。

### 实例归一化（Instance Normalization）

- **归一化范围**：特征维度（C）、空间维度（H, W），但不跨样本（N）。
- **应用**：对每个样本的每个通道进行独立归一化。
- **特点**：在训练和测试期间行为一致，适用于图像风格迁移等任务。

### 组归一化（Group Normalization）

- **归一化范围**：组内的特征维度（C）、空间维度（H, W），但不跨样本（N）。
- **应用**：将特征维度分成多个组，对每个组内的特征进行归一化处理。
- **特点**：在训练和测试期间行为一致，适用于批次大小较小或变化较大的情况。