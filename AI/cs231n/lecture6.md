<center>lecture6 CNN架构</center>

# 批量归一化

该方法具有一定的重要性，

1. 输入$x$未以0未中心（需要大的偏置调整）
2. 输入$x$的不同元素缩放不一致。
   如果输入数据的不同元素的尺度差异很大，权重矩阵的各个元素需要相应的变化很大，会使得训练过程更加复杂和不稳定

通常插入在FC和tanh之间，在tanh之前。

![image-20240729205721041](D:\zjPhD\notes\notes\AI\cs231n\图片\25.png)

# 层归一化

* 归一化是在每个样本的特征维度上进行的，依赖于每个样本的数据。

* 训练和测试阶段处理相同。

![image-20240729205820673](D:\zjPhD\notes\notes\AI\cs231n\图片\26.png)

# 实例归一化

![image-20240729210150064](D:\zjPhD\notes\notes\AI\cs231n\图片\27.png)

# 不同归一化方法的对比

在卷积神经网络中，我们有很多规划化的方法，包括批量归一化（Batch Normalization）、层归一化（Layer Normalization）、实例归一化（Instance Normalization）和组归一化（Group Normalization）。

### 批量归一化（Batch Normalization）

- **归一化范围**：批量维度（N）和空间维度（H, W）。
- **应用**：对整个批次的数据进行归一化处理。
- **特点**：在训练时依赖当前批次的均值和方差，测试时使用训练时的累计均值和方差。

### 层归一化（Layer Normalization）

- **归一化范围**：特征维度（C）和空间维度（H, W）。
- **应用**：对每个样本的所有特征进行归一化处理。
- **特点**：在训练和测试期间行为一致，不受批次大小的影响。

### 实例归一化（Instance Normalization）

- **归一化范围**：特征维度（C）、空间维度（H, W），但不跨样本（N）。
- **应用**：对每个样本的每个通道进行独立归一化。
- **特点**：在训练和测试期间行为一致，适用于图像风格迁移等任务。

### 组归一化（Group Normalization）

- **归一化范围**：组内的特征维度（C）、空间维度（H, W），但不跨样本（N）。
- **应用**：将特征维度分成多个组，对每个组内的特征进行归一化处理。
- **特点**：在训练和测试期间行为一致，适用于批次大小较小或变化较大的情况。



# 批量归一化

![image-20240804210333389](D:\zjPhD\notes\notes\AI\cs231n\图片\28.png)

引入两个科学系的参数。

批量归一化在测试阶段使用训练期间的滑动平均值（均值和方差）来进行归一化。这确保了模型在测试阶段的稳定性和一致性，并且批量归一化层在测试阶段变成了一个线性操作，可以与其他层融合以优化计算效率。这张图详细地展示了测试阶段批量归一化的计算过程和公式。

批量归一化的层通常插入再全连接层或者卷积层后，但是再非线性层前（Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity）

## 优点

**使深度网络更容易训练**：

- 批量归一化通过标准化每一层的激活值，使得网络训练更加稳定，避免了梯度消失和梯度爆炸的问题。

**改善梯度流**：

- 批量归一化可以保持梯度的适当范围，从而使得梯度能够更好地反向传播，提高模型的训练效率。

**允许更高的学习率，快速收敛**：

- 因为批量归一化能够稳定训练过程，使用更高的学习率成为可能，从而加快收敛速度。

**使网络对初始化更具鲁棒性**：

- 批量归一化减少了对权重初始化的敏感性，使得网络对不同的初始化方法更具鲁棒性。

**在训练过程中起到正则化的作用**：

- 批量归一化引入了一些噪声，因为每个小批量的数据统计量略有不同，这起到了类似于正则化的作用，有助于防止过拟合。

# 案例学习

## AlexNet

Input: 227x227x3 images

First layer (CONV1): 96 11x11 filters applied at stride 4：

Output volume [55x55x96]

Parameters: (11*11*3 + 1)*96 = 35K

Second layer (POOL1): 3x3 filters applied at stride 2

Q: what is the output volume size? Hint: (55-3)/2+1 = 27

Parameters: 0!

## VGGNet

AlexNet有8层，VGGNet则有16到19层。

VGG16和VGG19分别表示具有16层和19层的VGGNet变体。

这里发现了一个问题，那就是3个$3\times 3$的conv，stride1的层与$7\times 7$的层由相同的感受视野，具体表现为：
$$
假设输入为32\times32\\
(32-3)/1+1 = 30\\
(30-3)/1+1=28\\
(28-3)/1+1=26\\

但是我们使用7\times 7的卷积核\\
那么有(32-7)/1+1 = 26
$$
所以我们能够发现视野是一样的

我们发现了这个，但是我们更加倾向于使用小的卷积核，是因为小的卷积核会使得我们的网络更深，更加具有非线性

![image-20240804220410018](D:\zjPhD\notes\notes\AI\cs231n\图片\29.png)