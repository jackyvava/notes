<center>lecture6 CNN架构</center>

# 批量归一化

该方法具有一定的重要性，

1. 输入$x$未以0未中心（需要大的偏置调整）
2. 输入$x$的不同元素缩放不一致。
   如果输入数据的不同元素的尺度差异很大，权重矩阵的各个元素需要相应的变化很大，会使得训练过程更加复杂和不稳定

通常插入在FC和tanh之间，在tanh之前。

![image-20240729205721041](D:\zjPhD\notes\notes\AI\cs231n\图片\25.png)

# 层归一化

* 归一化是在每个样本的特征维度上进行的，依赖于每个样本的数据。

* 训练和测试阶段处理相同。

![image-20240729205820673](D:\zjPhD\notes\notes\AI\cs231n\图片\26.png)

# 实例归一化

![image-20240729210150064](D:\zjPhD\notes\notes\AI\cs231n\图片\27.png)

# 不同归一化方法的对比

在卷积神经网络中，我们有很多规划化的方法，包括批量归一化（Batch Normalization）、层归一化（Layer Normalization）、实例归一化（Instance Normalization）和组归一化（Group Normalization）。

### 批量归一化（Batch Normalization）

- **归一化范围**：批量维度（N）和空间维度（H, W）。
- **应用**：对整个批次的数据进行归一化处理。
- **特点**：在训练时依赖当前批次的均值和方差，测试时使用训练时的累计均值和方差。

### 层归一化（Layer Normalization）

- **归一化范围**：特征维度（C）和空间维度（H, W）。
- **应用**：对每个样本的所有特征进行归一化处理。
- **特点**：在训练和测试期间行为一致，不受批次大小的影响。

### 实例归一化（Instance Normalization）

- **归一化范围**：特征维度（C）、空间维度（H, W），但不跨样本（N）。
- **应用**：对每个样本的每个通道进行独立归一化。
- **特点**：在训练和测试期间行为一致，适用于图像风格迁移等任务。

### 组归一化（Group Normalization）

- **归一化范围**：组内的特征维度（C）、空间维度（H, W），但不跨样本（N）。
- **应用**：将特征维度分成多个组，对每个组内的特征进行归一化处理。
- **特点**：在训练和测试期间行为一致，适用于批次大小较小或变化较大的情况。



# 批量归一化

![image-20240804210333389](D:\zjPhD\notes\notes\AI\cs231n\图片\28.png)

引入两个科学系的参数。

批量归一化在测试阶段使用训练期间的滑动平均值（均值和方差）来进行归一化。这确保了模型在测试阶段的稳定性和一致性，并且批量归一化层在测试阶段变成了一个线性操作，可以与其他层融合以优化计算效率。这张图详细地展示了测试阶段批量归一化的计算过程和公式。

批量归一化的层通常插入再全连接层或者卷积层后，但是再非线性层前（Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity）

## 优点

**使深度网络更容易训练**：

- 批量归一化通过标准化每一层的激活值，使得网络训练更加稳定，避免了梯度消失和梯度爆炸的问题。

**改善梯度流**：

- 批量归一化可以保持梯度的适当范围，从而使得梯度能够更好地反向传播，提高模型的训练效率。

**允许更高的学习率，快速收敛**：

- 因为批量归一化能够稳定训练过程，使用更高的学习率成为可能，从而加快收敛速度。

**使网络对初始化更具鲁棒性**：

- 批量归一化减少了对权重初始化的敏感性，使得网络对不同的初始化方法更具鲁棒性。

**在训练过程中起到正则化的作用**：

- 批量归一化引入了一些噪声，因为每个小批量的数据统计量略有不同，这起到了类似于正则化的作用，有助于防止过拟合。

# 案例学习

## AlexNet

Input: 227x227x3 images

First layer (CONV1): 96 11x11 filters applied at stride 4：

Output volume [55x55x96]

Parameters: (11*11*3 + 1)*96 = 35K

Second layer (POOL1): 3x3 filters applied at stride 2

Q: what is the output volume size? Hint: (55-3)/2+1 = 27

Parameters: 0!

## VGGNet

AlexNet有8层，VGGNet则有16到19层。

VGG16和VGG19分别表示具有16层和19层的VGGNet变体。

这里发现了一个问题，那就是3个$3\times 3$的conv，stride1的层与$7\times 7$的层由相同的感受视野，具体表现为：
$$
假设输入为32\times32\\
(32-3)/1+1 = 30\\
(30-3)/1+1=28\\
(28-3)/1+1=26\\

但是我们使用7\times 7的卷积核\\
那么有(32-7)/1+1 = 26
$$
所以我们能够发现视野是一样的

我们发现了这个，但是我们更加倾向于使用小的卷积核，是因为小的卷积核会使得我们的网络更深，更加具有非线性

![image-20240804220410018](D:\zjPhD\notes\notes\AI\cs231n\图片\29.png)

## ResNet

使用残差连接的非常深的网络

当我们在普通的卷积神经网络上继续堆叠更深的层时候会出现“56层模型在测试误差和训练误差上表现更差”

更深的模型表现更差，但这不是由过拟合引起的！

深层模型的确具有更好的表现能力，但是在实际训练中，由于测u你在梯度消失或者梯度爆炸的原因，深层模型往往难以达到预期的性能。

解决方案：

1. 复制浅层模型的学习层
2. ==附加层设置为恒等映射==

![image-20240806155640713](D:\zjPhD\notes\notes\AI\cs231n\图片\30.png)

![image-20240806160336860](D:\zjPhD\notes\notes\AI\cs231n\图片\31.png)

用于ImageNet的总层数为18、34、50、101或152层

对于更深的网络（ResNet-50+），使用“瓶颈”层来提高效率（类似于GoogLeNet）:

* 1x1卷积，256个滤波器投射回256个特征图（28x28x256）
* 3x3卷积仅处理64个特征图
* 1x1卷积，64个滤波器投射到28x28x64

ResNet可以有不同的深度，这些不同深度的网络在ImageNet数据集上都表现良好。常见的ResNet深度有18层、34层、50层、101层和152层。更深的网络通常能够学习到更复杂的特征，但也需要更高的计算资源。

***瓶颈层***

对于更深的ResNet（50层及以上），引入了瓶颈层以提高计算效率。瓶颈层的设计灵感来自于GoogLeNet的Inception模块，通过减少卷积操作的计算量来提高效率。

#### 瓶颈层的结构

瓶颈层包含三个卷积层：

1. **1x1卷积（64个滤波器）**：将输入特征图的深度减少到64。
2. **3x3卷积（64个滤波器）**：在特征图上进行卷积操作。
3. **1x1卷积（256个滤波器）**：将特征图的深度增加回256。

Training ResNet in practice:
-Batch Normalization after every CONV layer
-Xavier initialization from He et al.
-SGD + Momentum (0.9)
-Learning rate: 0.1, divided by 10 when validation error plateaus
-Mini-batch size 256
-Weight decay of 1e-5
-No dropout used

## 主要收获

- **AlexNet**展示了你可以使用CNNs来训练计算机视觉模型。

- **VGG**展示了更大的网络效果更好。

- ResNet

  教会了我们如何训练极深的网络：

  - 只受到GPU和内存的限制！
  - 随着网络变大，收益递减。

- ResNet之后：CNNs的性能超越了人类的标准，研究重点转向了其他主题：

  - **高效网络**：MobileNet, ShuffleNet
  - **神经架构搜索**：现在可以自动化架构设计

==<center>Transfer Learning</center>==

# 迁移学习

You need a lot of a data if you want to train/use CNNs?

迁移学习是指在一个任务中学到的知识应用到另一个相关的任务中。在这里，可以现在大型数据集上训练一个深度的神经网络，然后将其引用在一个较小的数据集上。

![image-20240806162618439](D:\zjPhD\notes\notes\AI\cs231n\图片\32.png)

## 迁移学习的步骤

**步骤1：在ImageNet上训练**

- 首先，在ImageNet这样的大型数据集上训练卷积神经网络。通过这个步骤，网络能够学习到很多通用的特征。

**步骤2：在小数据集上微调**

- 当有一个较小的数据集时，可以利用在ImageNet上训练好的网络进行微调。
- **重新初始化并训练这个（FC-C）**：将网络的最后一层重新初始化，并针对小数据集的类别（C类）进行训练。
- **冻结这些层**：保持前面的卷积层和池化层的权重不变，因为这些层已经学到了通用的特征，不需要重新训练。

**步骤3：在较大数据集上微调**

- 对于较大数据集，可以训练更多的层以获取更好的表现。
- **训练这些层（FC-C, FC-4096, FC-4096）**：针对新的大数据集，重新训练网络的全连接层。
- ==**冻结这些层**==：保持前面的卷积层和池化层的权重不变。
- **使用较低的学习率**：在微调时，使用较小的学习率通常能得到更好的效果，原始学习率的1/10是一个好的起点。

> 通过迁移学习，可以显著减少训练时间，并提高模型在新任务上的表现。它充分利用了在大型数据集上学到的特征，使得在较小或不同的数据集上训练更加高效和准确。

| 3            | 非常相似的数据集     | 非常不同的数据集                             |
| ------------ | -------------------- | -------------------------------------------- |
| 数据量非常少 | 在顶层使用线性分类器 | 你会遇到麻烦... 尝试在不同阶段使用线性分类器 |
| 数据量相当多 | 微调一些层           | 微调大量层或从头开始训练                     |

**数据量非常少**：

- **非常相似的数据集**：如果新的数据集与原始数据集非常相似，可以直接在顶层使用线性分类器进行分类。因为底层和中层的特征已经足够描述新的数据集。
- **非常不同的数据集**：如果新的数据集与原始数据集差异很大，这种情况下直接使用顶层分类器可能效果不好。可以尝试在不同的中间层使用线性分类器，找到效果最好的层。

**数据量相当多**：

- **非常相似的数据集**：可以微调网络的顶层（例如全连接层）和一些中间层。这种策略能够在新的数据集上取得较好的效果，同时不需要重新训练整个网络。
- **非常不同的数据集**：需要微调更多的层，甚至从头开始训练整个网络。这样可以确保网络能够学习到新数据集的特征。

![image-20240806163648320](D:\zjPhD\notes\notes\AI\cs231n\图片\33.png)

## 项目和未来工作的关键策略

当你有一个相对较小的数据集（图像少于100万张）时，可以采用以下策略来提升模型性能：

1. **使用大数据集进行初始训练**：
   - 找到一个包含与目标数据集相似数据的非常大的数据集。
   - 在这个大数据集上训练一个深度神经网络模型。这样模型可以学习到丰富的特征表示。
2. **迁移学习到目标数据集**：
   - 将预训练的模型迁移到你的较小数据集上进行微调。通过这种方式，可以充分利用在大数据集上学到的特征，提高模型在较小数据集上的性能。

### 使用模型库（Model Zoo）

深度学习框架（如TensorFlow和PyTorch）提供了预训练模型的“模型库”（Model Zoo）。这些预训练模型已经在大型数据集（如ImageNet）上训练好，可以直接用于迁移学习。

- **TensorFlow的模型库**：[TensorFlow Model Zoo](https://github.com/tensorflow/models)
- **PyTorch的模型库**：[PyTorch Model Zoo](https://github.com/pytorch/vision)

通过使用这些预训练模型，可以节省大量的时间和计算资源，不需要从头开始训练模型。

## 总结

对于较小的数据集，采用迁移学习策略是一个有效的方法。首先在大型数据集上训练模型，然后将模型迁移到较小数据集上进行微调。同时，可以利用深度学习框架提供的预训练模型库，加速模型训练过程并提高性能。





<center>lecture6_part2 训练神经网络</center>

# 总览

1. 一次性设置

- **激活函数（activation functions）**：选择并设置适合的激活函数，如ReLU、Sigmoid、Tanh等。
- **预处理（preprocessing）**：对数据进行预处理，包括标准化、归一化、数据增强等，以提高模型的训练效果。
- **权重初始化（weight initialization）**：设置初始权重，常用的方法有Xavier初始化、He初始化等，以避免梯度消失或爆炸。
- **正则化（regularization）**：采用正则化技术如L2正则化、L1正则化或Dropout，防止模型过拟合。
- **梯度检查（gradient checking）**：验证反向传播算法的正确性，以确保梯度计算没有错误。

2. 训练动态

- **监控学习过程（babysitting the learning process）**：实时监控训练过程中的各项指标，如损失函数、准确率等，确保训练正常进行。
- **参数更新（parameter updates）**：选择和设置优化算法，如SGD、Adam、RMSprop等，以更新模型参数。
- **超参数优化（hyperparameter optimization）**：调整超参数，如学习率、批量大小、正则化参数等，以提高模型性能。

3. 评估

- **模型集成（model ensembles）**：通过集成多个模型（如投票、平均等）提高预测性能和鲁棒性。
- **测试时增强（test-time augmentation）**：在测试时对输入数据进行多种增强操作，并对多次预测结果进行平均，以提高模型的泛化能力。
- **迁移学习（transfer learning）**：利用预训练模型，将其迁移到新的数据集或任务上，通过微调提高模型的性能和训练效率。

## 激活函数

1. **Sigmoid**：

   - 优点：输出值在(0,1)之间，适用于输出概率的场景。
   - 缺点：容易导致梯度消失，特别是在深层网络中。

2. **tanh**：

   - 优点：输出值在(-1,1)之间，中心对称，有助于加速收敛。
   - 缺点：也容易导致梯度消失问题。

3. **ReLU**：

   - 优点：计算简单，收敛速度快（在实际应用中收敛速度比sigmoid/tanh快得多（例如快6倍）），缓解了梯度消失问题。
   - 缺点：在训练过程中可能会导致部分神经元“死亡”，即输出恒为0。
                输出不是零中心的
                当x<0，当输入小于等于0时，ReLU 的输出恒为0，其梯度也为0。这可能导致某些神经元在训练过程中“死亡”，即这些神经元再也不会更新其参数，影响模型的学习能力

   

4. **Leaky ReLU**：

   - 优点：解决了ReLU的“死亡”神经元问题。
   - 缺点：需要额外的参数。

5. **Maxout**：

   - 优点：具有很强的表示能力，可以拟合任意凸函数。
   - 缺点：需要更多的参数和计算资源。

6. **ELU**：

   - 优点：在负值区域有较好的特性，缓解了梯度消失问题。
   - 缺点：计算相对复杂。

> ==Sigmoid的梯度消失问题==
>
> 当输入值极大或极小时，Sigmoid函数的输出接近0或1，此时梯度接近于0，导致反向传播时梯度更新非常慢,甚至停止更新，这就是所谓的梯度消失问题

#  数据预处理

几种常见的图像预处理方法：

1. 减去均值图像（AlexNet等早期模型）
2. 减去每个通道均值（VGGNet等模型）
3. 减去每个通道均值并除以每通道标准差（ResNet及之后的模型）

> 图像预处理是深度学习中非常重要的一步，不同的预处理方法适用于不同的模型和数据集。通过中心化和标准化处理，可以显著提升模型的训练效果和稳定性

## 权重初始化

权重初始化对网络训练和收敛速度有很大影响。

![image-20240807145928666](D:\zjPhD\notes\notes\AI\cs231n\图片\34.png)

激活值取向0会导致网络中的==梯度消失问题==

从图中可以看出，在Layer 1，激活值的标准差为0.49，但到了Layer 6，标准差已经降到了0.05。这意味着激活值在传播过程中逐渐变小，几乎没有激活信号传递到更深层。

### Xavier 

Xavier 初始化方法（也称为 Glorot 初始化）是一种权重初始化策略，旨在使得神经网络的激活值在所有层中保持适当的尺度。这种方法的核心思想是让每一层的输入和输出的方差相同，从而避免梯度消失或梯度爆炸问题。

==没有看太懂这部分内容。$p_{39-50}$==

但是权重初始化是一件比较重要的研究领域

<font color='orange'> 一直再保持持续的研究</font>

![image-20240807153817728](D:\zjPhD\notes\notes\AI\cs231n\图片\35.png)

# 训练和测试误差

好的优化算法可以减少降低误差

但是我们比较关心在新的数据上的误差

## 提前停止

提前停止（Early Stopping）是指在验证集上的准确率开始下降时停止训练模型，或者在训练很长时间后，保留在验证集上表现最好的模型快照。

## 模型集成 (Model Ensembles)

1. **训练多个独立的模型**
   * 使用相同的数据集和不同的随机初始化或不同的超参数配置，训练多个模型。
   * 每个模型都是独立训练的，因此它们可能会捕捉到数据中的不同特征和模式。
2. 在测试时平均它们的结果
   - 计算每个模型预测的概率分布的平均值，然后选择最大值对应的类别。
   - **在测试时平均它们的结果**：
     - 对于每个输入数据点，所有模型都会进行预测，并给出相应的概率分布。
     - 通过对这些概率分布进行平均，可以得到综合的概率分布。
     - 最终，选择概率最高的类别作为预测结果（取平均后的概率分布中的最大值）

**模型集成通常可以带来额外的性能提升，实际应用中常见的提升幅度大约在 2% 左右。**

如何提高单个模型的表现？

==增加正则项==

正则化是一种防止过拟合的技术，通过在损失函数中假如额外的项$\lambda R(W)$，对模型的复杂度进行约束，使得模型具有更好的泛化性。

常用的正则化项：

1. L2正则化，权重衰退
   $$ R(W) = \Sigma_k\Sigma_l W_{k,l}^2$$
   通过惩罚较大的权重值，迫使模型参数尽量小，从而简化模型，防止过拟合

2. L1正则化，
   $$
   R(W)=\Sigma_k\Sigma_l |W_{k,l}|
   $$
   通过惩罚权重的绝对值，使部分权重趋于零，从而实现特征选择和稀疏化模型。

3. 弹性网（L1 + L2）:
   $$
   R(W)=\Sigma_k\Sigma_l\beta W_{k,l}^2+|W_{k,l}|
   $$
   结合 L1 和 L2 正则化的优点，既可以惩罚大权重，又可以实现稀疏化，提高模型的泛化能力。

### Dropout

在每一次前向传递中，随机将一些神经元的输出设为零，丢弃的概率是一个超参数，通常设为0.5。

Dropout 是一种有效的正则化技术，用于防止神经网络的过拟合。其基本思想是，在训练过程中随机地将一部分神经元的输出设为零，以此减少神经元之间的相互依赖，提高模型的泛化能力。

```python
p = 0.5  # 保持神经元激活的概率。值越高，丢弃的神经元越少。

def train_step(X):
    """ X 包含输入数据 """
    
    # 3层神经网络的前向传递
    H1 = np.maximum(0, np.dot(W1, X) + b1)
    U1 = np.random.rand(*H1.shape) < p  # 第一层的 dropout 掩码
    H1 *= U1  # 丢弃第一层的一部分神经元
    
    H2 = np.maximum(0, np.dot(W2, H1) + b2)
    U2 = np.random.rand(*H2.shape) < p  # 第二层的 dropout 掩码
    H2 *= U2  # 丢弃第二层的一部分神经元
    
    out = np.dot(W3, H2) + b3
    
    # 反向传播计算梯度（未显示）
    # 更新参数（未显示）

```

随机丢弃一些特征，使得网络依赖其他的特征来进行打分，能够在一定程度上确保网络的泛化能力

我们训练可以有整体的所有特征，使用Dropout实际上是在每一个训练步骤时，选择特征的不同子集来完成，这样做的好处就是能够提高模型的泛化能力。

假如一个全连接层有4096个单元，那么使用0或者1来表示是否使用这个掩码，就是$2^{4096}\~10^{1233}$种可能，但是宇宙中大概只有$10^{82}$原子。

这里主要介绍的就是Dropout正则化技术：

> 在Dropout正则化技术中，将激活值乘以训练时神经元保持激活的概率 ppp 这一步骤，在测试时通常被称为**“反向Dropout”**（Inverted Dropout）。
>
> 这种方法是指在训练时，不仅通过Dropout随机地关闭一些神经元，而且在激活时会将剩余激活的神经元的输出除以保持激活的概率 ppp，这样可以保证无论Dropout掩码如何变化，输出的总体期望值保持不变。这样做的好处是在测试时不需要对神经网络进行任何额外的缩放操作，因为所有权重已经适应了Dropout的影响。

所以正则化，可以这么理解，==在训练的时候，引入一些随机性，而在测试的时候，平均掉这些随机性（有时候可能需要近似）==

训练：
$$
y=f_W(x,z)，这里z就引入了随机变量
$$
测试：
$$
y=f(x) = E_z[f(x,z)]=\int p(x)f(x,z)dz
$$
![image-20240808151241682](D:\zjPhD\notes\notes\AI\cs231n\图片\36.png)

在训练阶段，通过随机裁剪和调整尺寸的方法可以增加模型看到的图像多样性，这有助于模型学习到更加鲁棒的特征，不会过度依赖于特定的图像大小或位置。

在测试阶段，通过在不同尺寸和固定的裁剪位置上平均模型的输出，可以确保模型对于图像的微小变动不敏感，提高了模型对新数据的预测稳定性。

## 总结

### 1. **Dropout**

- **应用场景**：主要用于大型全连接层。
- **方法概述**：通过随机丢弃网络中的部分神经元（即设置为零），防止模型对特定神经元的依赖，从而减少过拟合。
- **训练时**：随机丢弃。
- **测试时**：所有神经元都参与，但每个神经元的输出可能会根据dropout比例进行缩放（如使用反向Dropout）。

### 2. **Batch Normalization**

- **应用场景**：广泛用于各种网络层中。
- **方法概述**：对每个小批量的数据进行标准化处理，使得输出的均值为0，方差为1。这有助于解决内部协变量偏移问题，加速训练过程。
- **训练时**：使用批内数据的统计特性。
- **测试时**：使用训练过程中累积（或移动平均）的统计特性。

### 3. **数据增强**

- **应用场景**：适用于图像等数据丰富的领域。
- **方法概述**：通过对原始图像应用各种变换（如随机裁剪、旋转、缩放等）来增加训练数据的多样性。
- **训练时**：应用随机变换。
- **测试时**：通常使用原始未变换的数据，或者使用一组固定的变换来评估模型。

### 4. **Cutout**

- **应用场景**：特别适用于小型图像分类数据集。
- **方法概述**：在图像中随机选择区域并将其像素设置为零，类似于局部的遮挡。
- **训练时**：应用随机的剪切。
- **测试时**：使用完整的图像。

# ==选择超参数的步骤==

**步骤 1: 检查初始损失**

- 确认模型在初始化时的损失值是否合理。例如，对于分类问题使用Softmax损失，初始损失应接近于 log⁡(C)\log(C)log(C)，其中 CCC 是类别数量。

**步骤 2: 过拟合小样本**

- 验证模型架构的有效性。尝试让模型在一个非常小的数据集上达到100%的训练精度。这有助于确保模型至少能够学习并记忆少量数据。

**步骤 3: 寻找使损失下降的学习率**

- 使用从上一步得到的架构，调整学习率来查看哪个学习率能有效地降低损失。常见的学习率值包括 1e−1,1e−2,1e−3,1e−41e-1, 1e-2, 1e-3, 1e-41e−1,1e−2,1e−3,1e−4。

**步骤 4: 粗糙网格搜索，训练约1-5个周期**

- 选取几个从步骤3中表现良好的学习率和权重衰减值，进行初步的模型训练，每个模型训练1-5个周期。

**步骤 5: 精细网格搜索，延长训练时间**

- 从步骤4中选择表现最佳的模型进行更长时间的训练，以精细调整和验证选定的超参数。

**步骤 6: 观察损失和精度曲线**

- 分析各个模型的训练和验证损失及精度曲线，以判断模型是否过拟合或欠拟合，从而对超参数进行最后的调整。

这个过程是迭代的，每一步都建立在前一步的基础上，通过不断调整和验证来逐步找到最佳的模型配置。这种方法确保了模型在实际应用中能够达到良好的泛化性能。

# 训练和验证精度随着时间变化

## 第一种情况

![image-20240808155921461](D:\zjPhD\notes\notes\AI\cs231n\图片\37.png)

这种情况还需要继续训练，精度还有提升的空间。

## 第二种情况

训练和验证准确率有较大差距

![image-20240808162246915](D:\zjPhD\notes\notes\AI\cs231n\图片\38.png)

**观察**：训练准确率远高于验证准确率，且两者之间的差距随时间增大

**含义**：这表明模型在==训练数据上表现很好==，但未能很好地泛化到未见过的数据上，即发生了==过拟合==。

**建议**：

* **增加正则化**：应用更强的正则化技术，如增加Dropout比例、使用L1或L2正则化等，以防止模型过于复杂。
* **获取更多数据**：增加数据量可以帮助模型学习更泛化的特征，减少对训练数据的特定依赖。
* **数据增强**：通过对训练数据进行变换（如旋转、缩放、剪切等），增加模型对输入数据的适应性。

## 第三种情况

![image-20240808162453770](D:\zjPhD\notes\notes\AI\cs231n\图片\39.png)

**观察**：训练和验证准确率彼此非常接近，并且几乎没有差距。

**含义**：模型在训练和验证数据上都有均衡的表现，==没有明显的过拟合或欠拟合迹象==。这可能表明==模型的复杂度相对较低==，未完全捕捉到数据中的所有可用模式。

**建议**：

* **训练更长时间**：如果模型的训练还未达到收敛，可以尝试延长训练时间，观察是否能进一步提高准确率。
* **尝试更大的模型**：使用更大或更复杂的网络结构可能有助于提高模型性能，特别是当现有模型似乎未能充分学习数据中的特征时。

![image-20240808163939122](D:\zjPhD\notes\notes\AI\cs231n\图片\40.png)

## 训练损失曲线

- **观察**：训练损失曲线显示在迭代过程中损失逐渐下降，并逐渐趋于稳定。图中的数据点比较杂乱，表明损失值在训练过程中波动

## 训练/验证准确率曲线

**观察**：训练准确率持续上升，而验证准确率在一定点后趋于平稳，两者之间存在一定的差距。

**含义**：这表明模型在训练数据上逐渐适应得更好，但在验证数据上的表现提升有限，显示出一定程度的过拟合。

# 超参数优化方法

两种超参数优化方法：网格搜索（Grid Search）和随机搜索（Random Search），并根据Bergstra和Bengio的2012年的研究进行了比较。

### 网格搜索 (Grid Search)

- **布局**：网格搜索通过在每个超参数的所有可能值上创建一个网格来系统地遍历所有可能的参数组合。

- 特点

  ：

  - **系统性**：每个参数组合都会被测试，保证了搜索的彻底性。
  - **效率问题**：在参数数量较多或参数可取值范围较广时，网格搜索的计算成本和时间成本会迅速增加。
  - **均匀分布**：所有参数都被平等对待，不考虑参数的重要性。

### 随机搜索 (Random Search)

- **布局**：随机搜索通过在参数的可能值范围内随机选择参数组合来进行搜索。

- 特点

  ：

  - **高效性**：随机搜索通常比网格搜索更快地找到性能良好的参数组合，特别是在某些参数明显比其他参数更重要的情况下。
  - **覆盖广**：随机选择的方式可能会探索到网格搜索未能覆盖的区域。
  - **灵活性**：易于扩展到更多的参数和更广的参数范围。

### 优势与劣势比较

- **网格搜索**的优势在于其系统性和完整性，适合参数数量较少且范围较小的情况。
- **随机搜索**在处理具有多个超参数和较大搜索空间的复杂模型时具有优势，因为它能更快地覆盖更广泛的空间，并且在实践中常常能更有效地找到优秀的解决方案。



# 训练网络的一些总结和建议

### 1. 激活函数：使用ReLU

- **ReLU (Rectified Linear Unit)**：是最常用的激活函数之一，因其在训练深层神经网络时带来的计算效率和稀疏激活特性而受到青睐。
- **优点**：有助于缓解梯度消失问题，加速神经网络的收敛速度，通常比其他激活函数（如sigmoid或tanh）效果更好。

### 2. 数据预处理：图像数据减去均值

- **减去均值**：这是图像预处理中的常用方法，通过从每个像素中减去整个训练集的平均像素值，可以使模型训练更稳定。
- **目的**：标准化数据，将输入特征的中心调整为零，通常可以提高模型的学习效率。

### 3. 权重初始化：使用Xavier/Kaiming初始化

- **Xavier初始化**：适用于与tanh或sigmoid激活函数配合使用的网络。
- **Kaiming初始化**：也称为He初始化，适用于与ReLU激活函数配合使用的网络。
- **目的**：适当的权重初始化可以防止在网络的前向和反向传播过程中出现激活值和梯度的爆炸或消失。

### 4. 批量归一化：使用这个

- **批量归一化（Batch Normalization）**：通过对每个小批量数据进行归一化处理，稳定了神经网络的训练过程，提高了学习速率和模型对初始权重的不敏感性。
- **效果**：常常能显著提高训练的稳定性，并允许使用更高的学习率，加速收敛。

### 5. 迁移学习：如果可能，使用这个

- **迁移学习**：利用在一个大型数据集（如ImageNet）上预训练的模型，调整或微调以适应一个新的、相对较小的数据集。
- **优点**：可以显著提高学习效率和预测性能，特别是当新数据有限时。