<center>Lecture 3 正则化和优化</center>

假设我们有N个样本，总共有$j$个类别，每个样本的正确类比为$y_i$，那么我们的总损失函数可以表示为：
$$
L = \frac{1}{N} \Sigma_i^N \Sigma_{j\neq y_i}(f(x_i,w)_j,f(x_i,w)_{y_i})
$$
损失函数不唯一。

![fig1](D:\zjPhD\notes\notes\AI\cs231n\图片\12.png)

那么我们如何去选择是用$w$还是$2w$？

我们选择正则化，正则化可以防止数据过拟合，考虑进噪音。
$$
L(W) = \frac{1}{N} \Sigma_i^N L_i(f(x_i,w)_j,y_i)+\lambda R(W)
$$
$\lambda$是正则化强度，一个超参数。使用正则化可以有效避免过拟合

简单的正则化案例：

$L_1$正则化，$R(W)= \Sigma_k \Sigma_l |W_{k,l}|$

$L_2$正则化，$R(W)= \Sigma_k \Sigma_l W_{k,l}^2$

弹性网络：结合$L_1$和$L_2$，$R(W) =\Sigma_k \Sigma_l \beta W_{l,l}^2+|W_{k,l}| $



为什么要进行正则化？

1. 表达对权重的偏好，比如L2倾向于使得权重值变小，使得模型更加平滑和简单，减少过拟合；而L1倾向于稀疏的权重，即使许多权重变为0，从而进行**特征选择**。
2. 使模型简单，从而在测试数据上表现良好
3. 通过增加曲率改善优化。正则化项会增加损失函数的曲率，使得损失函数更加凸，从而改善优化过程。

案例说明

输入向量 $$x = [1, 1, 1, 1] $$，两个权重向量： 

$$ w_1 = [1, 0, 0, 0] $$ 

$$w_2 = [0.25, 0.25, 0.25, 0.25] $$

无论是$ w_1 $ 还是$ w_2 $，当它们与输入向量$ x $ 相乘时，结果都是 1：

$$ w_1^T x = w_2^T x = 1 $$

其中，$L_2$正则化更加倾向于”分散权重“，使得权重较小且分布均匀。
$$
R(\mathbf{w}_1) = 1^2 + 0^2 + 0^2 + 0^2 = 1
$$

$$
R(\mathbf{w}_2) = 0.25^2 + 0.25^2 + 0.25^2 + 0.25^2 = 0.0625 + 0.0625 + 0.0625 + 0.0625 = 0.25
$$


$L_1$ 正则化更倾向于稀疏的权重，使得许多权重变为0.
$$
R(\mathbf{w}_1) = |1| + |0| + |0| + |0| = 1
$$

$$
R(\mathbf{w}_2) = |0.25| + |0.25| + |0.25| + |0.25| = 1
$$
所以，L_1更加倾向于选择$w_1$，而$L_2$更加倾向于选择$w_2$

- **L2正则化**：在上图中，L2正则化会偏好($\mathbf{w}_2 = [0.25, 0.25, 0.25, 0.25]$，因为它“分散”了权重，减少了正则化项的值。

- **L1正则化**：L1正则化会偏好$ \mathbf{w}_1 = [1, 0, 0, 0]$ ，因为它是稀疏的，符合L1正则化的稀疏性偏好。 

![image-20240715215354896](D:\zjPhD\notes\notes\AI\cs231n\图片\13.png)

现在的问题是我们如何找到最好的W？

引出优化$(Optimization)$

第一种是随机搜索，这种似乎正确率也还可以

```python
bestloss 
for num in range(10000):
    w = np.random(10,3072)
    loss = L(x_train,y_train,W)
    if loss < bestloss:
        bestloss = loss
        bestW = w
    
```

第二种就是沿着梯度斜坡

梯度$\mathbf \nabla f(x)$是函数增长最快的方向

所以一般选取负梯度

![image-20240716011941126](D:\zjPhD\notes\notes\AI\cs231n\图片\14.png)

这种方法是使用数值梯度，非常慢，需要循环所有的维度，而且是近似的

实际上损失函数是权重$W$的函数，可以用微积分来计算$\nabla_W L$

这里$dW = some\ function\ data\ and\ W$

比如我们可以选取前面提到的$L_1$$L_2$正则化这类。

* {数值梯度}}：近似的、慢的、容易编写
* {解析梯度}}：精确的、快的、容易出错

在实践中：总是使用解析梯度，但通过数值梯度来检查实现。这被称为梯度检查



随机梯度下降$(SGD)$
$$
L(W) =\frac1N\sum_{i=1}^NL_i(x_i,y_i,W)+\lambda R(W) \\
\nabla_{W}L(W) =\frac1N\sum_{i=1}^N\nabla_WL_i(x_i,y_i,W)+\lambda\nabla_WR(W) 
$$
当N很大的时候，进行全求和很浪费资源

因此我们使用小批量（minibatch）样本来近似求和，常见的小批量大小为32、64、128

```python
while True:
    data_batch = sample_training_data(data, 256)  # sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += - step_size * weights_grad  # perform parameter update
```

SGD 存在的问题：

如果损失函数在一个方向上变化很快，再另一个方向上很慢，会出现什么情况

* 在这种情况下，梯度下降在浅维度上进展非常慢，而在陡峭方向上会抖动。

损失函数具有高条件数：Hessian矩阵的最大奇异值与最小奇异值比率很大。

![image-20240716014640740](D:\zjPhD\notes\notes\AI\cs231n\图片\15.png)

SGD 问题2，容易陷入局部最小值和鞍点

SGD问题3，通常使用小批量样本来近似梯度，可能受限于样本选择的影响，包含较大的噪声，这样的噪声下的路径是不稳定的



对于上述解决办法，采用SGD+动量的方法

![image-20240716024655857](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240716024655857.png)

1. **加速收敛**：动量法可以加速收敛速度，尤其是在鞍点或平坦区域，使得优化过程更快地到达最优解。
2. **避免局部最优**：动量法可以帮助算法摆脱局部最优，继续向全局最优解搜索。
3. **减少抖动**：通过累积梯度信息，动量法可以减少参数更新过程中的抖动，使得优化路径更加平滑稳定。



SGD加动量有两种写法，要适应，见P66



还有更加复杂的优化方法RMSProp:引入历史梯度平方和的移动平均来调整每个维度的梯度更新步长