<center>Lecture 3 æ­£åˆ™åŒ–å’Œä¼˜åŒ–</center>

å‡è®¾æˆ‘ä»¬æœ‰Nä¸ªæ ·æœ¬ï¼Œæ€»å…±æœ‰$j$ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ­£ç¡®ç±»æ¯”ä¸º$y_i$ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„æ€»æŸå¤±å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$
L = \frac{1}{N} \Sigma_i^N \Sigma_{j\neq y_i}(f(x_i,w)_j,f(x_i,w)_{y_i})
$$
æŸå¤±å‡½æ•°ä¸å”¯ä¸€ã€‚

![fig1](D:\zjPhD\notes\notes\AI\cs231n\å›¾ç‰‡\12.png)

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å»é€‰æ‹©æ˜¯ç”¨$w$è¿˜æ˜¯$2w$ï¼Ÿ

æˆ‘ä»¬é€‰æ‹©æ­£åˆ™åŒ–ï¼Œæ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢æ•°æ®è¿‡æ‹Ÿåˆï¼Œè€ƒè™‘è¿›å™ªéŸ³ã€‚
$$
L(W) = \frac{1}{N} \Sigma_i^N L_i(f(x_i,w)_j,y_i)+\lambda R(W)
$$
$\lambda$æ˜¯æ­£åˆ™åŒ–å¼ºåº¦ï¼Œä¸€ä¸ªè¶…å‚æ•°ã€‚ä½¿ç”¨æ­£åˆ™åŒ–å¯ä»¥æœ‰æ•ˆé¿å…è¿‡æ‹Ÿåˆ

ç®€å•çš„æ­£åˆ™åŒ–æ¡ˆä¾‹ï¼š

$L_1$æ­£åˆ™åŒ–ï¼Œ$R(W)= \Sigma_k \Sigma_l |W_{k,l}|$

$L_2$æ­£åˆ™åŒ–ï¼Œ$R(W)= \Sigma_k \Sigma_l W_{k,l}^2$

å¼¹æ€§ç½‘ç»œï¼šç»“åˆ$L_1$å’Œ$L_2$ï¼Œ$R(W) =\Sigma_k \Sigma_l \beta W_{l,l}^2+|W_{k,l}| $



ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ­£åˆ™åŒ–ï¼Ÿ

1. è¡¨è¾¾å¯¹æƒé‡çš„åå¥½ï¼Œæ¯”å¦‚L2å€¾å‘äºä½¿å¾—æƒé‡å€¼å˜å°ï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ å¹³æ»‘å’Œç®€å•ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼›è€ŒL1å€¾å‘äºç¨€ç–çš„æƒé‡ï¼Œå³ä½¿è®¸å¤šæƒé‡å˜ä¸º0ï¼Œä»è€Œè¿›è¡Œ**ç‰¹å¾é€‰æ‹©**ã€‚
2. ä½¿æ¨¡å‹ç®€å•ï¼Œä»è€Œåœ¨æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½
3. é€šè¿‡å¢åŠ æ›²ç‡æ”¹å–„ä¼˜åŒ–ã€‚æ­£åˆ™åŒ–é¡¹ä¼šå¢åŠ æŸå¤±å‡½æ•°çš„æ›²ç‡ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æ›´åŠ å‡¸ï¼Œä»è€Œæ”¹å–„ä¼˜åŒ–è¿‡ç¨‹ã€‚

æ¡ˆä¾‹è¯´æ˜

è¾“å…¥å‘é‡ $$x = [1, 1, 1, 1] $$ï¼Œä¸¤ä¸ªæƒé‡å‘é‡ï¼š 

$$ w_1 = [1, 0, 0, 0] $$ 

$$w_2 = [0.25, 0.25, 0.25, 0.25] $$

æ— è®ºæ˜¯$ w_1 $ è¿˜æ˜¯$ w_2 $ï¼Œå½“å®ƒä»¬ä¸è¾“å…¥å‘é‡$ x $ ç›¸ä¹˜æ—¶ï¼Œç»“æœéƒ½æ˜¯ 1ï¼š

$$ w_1^T x = w_2^T x = 1 $$

å…¶ä¸­ï¼Œ$L_2$æ­£åˆ™åŒ–æ›´åŠ å€¾å‘äºâ€åˆ†æ•£æƒé‡â€œï¼Œä½¿å¾—æƒé‡è¾ƒå°ä¸”åˆ†å¸ƒå‡åŒ€ã€‚
$$
R(\mathbf{w}_1) = 1^2 + 0^2 + 0^2 + 0^2 = 1
$$

$$
R(\mathbf{w}_2) = 0.25^2 + 0.25^2 + 0.25^2 + 0.25^2 = 0.0625 + 0.0625 + 0.0625 + 0.0625 = 0.25
$$


$L_1$ æ­£åˆ™åŒ–æ›´å€¾å‘äºç¨€ç–çš„æƒé‡ï¼Œä½¿å¾—è®¸å¤šæƒé‡å˜ä¸º0.
$$
R(\mathbf{w}_1) = |1| + |0| + |0| + |0| = 1
$$

$$
R(\mathbf{w}_2) = |0.25| + |0.25| + |0.25| + |0.25| = 1
$$
æ‰€ä»¥ï¼ŒL_1æ›´åŠ å€¾å‘äºé€‰æ‹©$w_1$ï¼Œè€Œ$L_2$æ›´åŠ å€¾å‘äºé€‰æ‹©$w_2$

- **L2æ­£åˆ™åŒ–**ï¼šåœ¨ä¸Šå›¾ä¸­ï¼ŒL2æ­£åˆ™åŒ–ä¼šåå¥½($\mathbf{w}_2 = [0.25, 0.25, 0.25, 0.25]$ï¼Œå› ä¸ºå®ƒâ€œåˆ†æ•£â€äº†æƒé‡ï¼Œå‡å°‘äº†æ­£åˆ™åŒ–é¡¹çš„å€¼ã€‚

- **L1æ­£åˆ™åŒ–**ï¼šL1æ­£åˆ™åŒ–ä¼šåå¥½$ \mathbf{w}_1 = [1, 0, 0, 0]$ ï¼Œå› ä¸ºå®ƒæ˜¯ç¨€ç–çš„ï¼Œç¬¦åˆL1æ­£åˆ™åŒ–çš„ç¨€ç–æ€§åå¥½ã€‚ 

![image-20240715215354896](D:\zjPhD\notes\notes\AI\cs231n\å›¾ç‰‡\13.png)

ç°åœ¨çš„é—®é¢˜æ˜¯æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°æœ€å¥½çš„Wï¼Ÿ

å¼•å‡ºä¼˜åŒ–$(Optimization)$

ç¬¬ä¸€ç§æ˜¯éšæœºæœç´¢ï¼Œè¿™ç§ä¼¼ä¹æ­£ç¡®ç‡ä¹Ÿè¿˜å¯ä»¥

```python
bestloss 
for num in range(10000):
    w = np.random(10,3072)
    loss = L(x_train,y_train,W)
    if loss < bestloss:
        bestloss = loss
        bestW = w
    
```

ç¬¬äºŒç§å°±æ˜¯æ²¿ç€æ¢¯åº¦æ–œå¡

æ¢¯åº¦$\mathbf \nabla f(x)$æ˜¯å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘

æ‰€ä»¥ä¸€èˆ¬é€‰å–è´Ÿæ¢¯åº¦

![image-20240716011941126](D:\zjPhD\notes\notes\AI\cs231n\å›¾ç‰‡\14.png)

è¿™ç§æ–¹æ³•æ˜¯ä½¿ç”¨æ•°å€¼æ¢¯åº¦ï¼Œéå¸¸æ…¢ï¼Œéœ€è¦å¾ªç¯æ‰€æœ‰çš„ç»´åº¦ï¼Œè€Œä¸”æ˜¯è¿‘ä¼¼çš„

å®é™…ä¸ŠæŸå¤±å‡½æ•°æ˜¯æƒé‡$W$çš„å‡½æ•°ï¼Œå¯ä»¥ç”¨å¾®ç§¯åˆ†æ¥è®¡ç®—$\nabla_W L$

è¿™é‡Œ$dW = some\ function\ data\ and\ W$

æ¯”å¦‚æˆ‘ä»¬å¯ä»¥é€‰å–å‰é¢æåˆ°çš„$L_1$$L_2$æ­£åˆ™åŒ–è¿™ç±»ã€‚

* {æ•°å€¼æ¢¯åº¦}}ï¼šè¿‘ä¼¼çš„ã€æ…¢çš„ã€å®¹æ˜“ç¼–å†™
* {è§£ææ¢¯åº¦}}ï¼šç²¾ç¡®çš„ã€å¿«çš„ã€å®¹æ˜“å‡ºé”™

åœ¨å®è·µä¸­ï¼šæ€»æ˜¯ä½¿ç”¨è§£ææ¢¯åº¦ï¼Œä½†é€šè¿‡æ•°å€¼æ¢¯åº¦æ¥æ£€æŸ¥å®ç°ã€‚è¿™è¢«ç§°ä¸ºæ¢¯åº¦æ£€æŸ¥



éšæœºæ¢¯åº¦ä¸‹é™$(SGD)$
$$
L(W) =\frac1N\sum_{i=1}^NL_i(x_i,y_i,W)+\lambda R(W) \\
\nabla_{W}L(W) =\frac1N\sum_{i=1}^N\nabla_WL_i(x_i,y_i,W)+\lambda\nabla_WR(W) 
$$
å½“Nå¾ˆå¤§çš„æ—¶å€™ï¼Œè¿›è¡Œå…¨æ±‚å’Œå¾ˆæµªè´¹èµ„æº

å› æ­¤æˆ‘ä»¬ä½¿ç”¨å°æ‰¹é‡ï¼ˆminibatchï¼‰æ ·æœ¬æ¥è¿‘ä¼¼æ±‚å’Œï¼Œå¸¸è§çš„å°æ‰¹é‡å¤§å°ä¸º32ã€64ã€128

```python
while True:
    data_batch = sample_training_data(data, 256)  # sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
    weights += - step_size * weights_grad  # perform parameter update
```

SGD å­˜åœ¨çš„é—®é¢˜ï¼š

å¦‚æœæŸå¤±å‡½æ•°åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šå˜åŒ–å¾ˆå¿«ï¼Œå†å¦ä¸€ä¸ªæ–¹å‘ä¸Šå¾ˆæ…¢ï¼Œä¼šå‡ºç°ä»€ä¹ˆæƒ…å†µ

* åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¸‹é™åœ¨æµ…ç»´åº¦ä¸Šè¿›å±•éå¸¸æ…¢ï¼Œè€Œåœ¨é™¡å³­æ–¹å‘ä¸Šä¼šæŠ–åŠ¨ã€‚

æŸå¤±å‡½æ•°å…·æœ‰é«˜æ¡ä»¶æ•°ï¼šHessiançŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼ä¸æœ€å°å¥‡å¼‚å€¼æ¯”ç‡å¾ˆå¤§ã€‚

![image-20240716014640740](D:\zjPhD\notes\notes\AI\cs231n\å›¾ç‰‡\15.png)

SGD é—®é¢˜2ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼å’Œéç‚¹

SGDé—®é¢˜3ï¼Œé€šå¸¸ä½¿ç”¨å°æ‰¹é‡æ ·æœ¬æ¥è¿‘ä¼¼æ¢¯åº¦ï¼Œå¯èƒ½å—é™äºæ ·æœ¬é€‰æ‹©çš„å½±å“ï¼ŒåŒ…å«è¾ƒå¤§çš„å™ªå£°ï¼Œè¿™æ ·çš„å™ªå£°ä¸‹çš„è·¯å¾„æ˜¯ä¸ç¨³å®šçš„



å¯¹äºä¸Šè¿°è§£å†³åŠæ³•ï¼Œé‡‡ç”¨SGD+åŠ¨é‡çš„æ–¹æ³•

![image-20240716024655857](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20240716024655857.png)

1. **åŠ é€Ÿæ”¶æ•›**ï¼šåŠ¨é‡æ³•å¯ä»¥åŠ é€Ÿæ”¶æ•›é€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨éç‚¹æˆ–å¹³å¦åŒºåŸŸï¼Œä½¿å¾—ä¼˜åŒ–è¿‡ç¨‹æ›´å¿«åœ°åˆ°è¾¾æœ€ä¼˜è§£ã€‚
2. **é¿å…å±€éƒ¨æœ€ä¼˜**ï¼šåŠ¨é‡æ³•å¯ä»¥å¸®åŠ©ç®—æ³•æ‘†è„±å±€éƒ¨æœ€ä¼˜ï¼Œç»§ç»­å‘å…¨å±€æœ€ä¼˜è§£æœç´¢ã€‚
3. **å‡å°‘æŠ–åŠ¨**ï¼šé€šè¿‡ç´¯ç§¯æ¢¯åº¦ä¿¡æ¯ï¼ŒåŠ¨é‡æ³•å¯ä»¥å‡å°‘å‚æ•°æ›´æ–°è¿‡ç¨‹ä¸­çš„æŠ–åŠ¨ï¼Œä½¿å¾—ä¼˜åŒ–è·¯å¾„æ›´åŠ å¹³æ»‘ç¨³å®šã€‚



SGDåŠ åŠ¨é‡æœ‰ä¸¤ç§å†™æ³•ï¼Œè¦é€‚åº”ï¼Œè§P66



è¿˜æœ‰æ›´åŠ å¤æ‚çš„ä¼˜åŒ–æ–¹æ³•RMSProp:å¼•å…¥å†å²æ¢¯åº¦å¹³æ–¹å’Œçš„ç§»åŠ¨å¹³å‡æ¥è°ƒæ•´æ¯ä¸ªç»´åº¦çš„æ¢¯åº¦æ›´æ–°æ­¥é•¿



æˆ‘ä»¬æœ€é•¿ç”¨çš„æ˜¯Adamä¼˜åŒ–

ç»™å‡ºäº†Adamä¼˜åŒ–ç®—æ³•çš„å‚æ•°å»ºè®®ï¼š

- `beta1 = 0.9`
- `beta2 = 0.999`
- `learning_rate = 1e-3` æˆ– `5e-4`

è¿™äº›å€¼é€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œå¯ä»¥é€‚ç”¨äºè®¸å¤šæ¨¡å‹ã€‚



è¿›ä¸€æ­¥çš„ï¼Œå¯ä»¥ä½¿ç”¨å¸¦æœ‰æƒé‡è¡°å‡çš„Adamã€‚

ä¸‹é¢æ˜¯æ ‡å‡†Adamç®—æ³•ï¼š

```python
first_moment = 0
second_moment = 0
for t in range(1, num_iterations):
    dx = compute_gradient(x)  # è®¡ç®—æ¢¯åº¦
    first_moment = beta1 * first_moment + (1 - beta1) * dx
    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx
    first_unbias = first_moment / (1 - beta1 ** t)
    second_unbias = second_moment / (1 - beta2 ** t)
    x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)

```

æ ‡å‡†Adamç®—æ³•ä¸­çš„$L_2$æ­£åˆ™åŒ–

```python
dx = compute_gradient(x) + lambda * x  # L2æ­£åˆ™åŒ–é¡¹ lambda * x

```

AdamWç®—æ³•

```python
x -= learning_rate * (first_unbias / (np.sqrt(second_unbias) + 1e-7) + weight_decay * x)

```

- **AdamWç®—æ³•çš„æƒé‡è¡°å‡**ï¼šæƒé‡è¡°å‡é¡¹åœ¨å‚æ•°æ›´æ–°æ—¶æ·»åŠ ã€‚è¿™ç§æ–¹å¼ä½¿å¾—æ­£åˆ™åŒ–é¡¹ä¸åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ä¼°è®¡å€¼æ— å…³ï¼Œä»è€Œæ›´åŠ å‡†ç¡®åœ°åº”ç”¨æƒé‡è¡°å‡ã€‚

ä¸Šè¿°çš„å‡ ç§ç®—æ³•éƒ½å°†å­¦ä¹ ç‡ä½œä¸ºäº†è¶…å‚æ•°ã€‚

å¯¹äºå­¦ä¹ ç‡çš„è®¾ç½®ï¼Œæœ‰ä¸€ç›´æ’å®šä¸å˜çš„ï¼Œä¹Ÿæœ‰è¡°å‡å­¦ä¹ ç‡çš„æƒ…å†µ

å¯¹äºè¡°å‡å­¦ä¹ ç‡ï¼šä¸¤ç§å¸¸è§çš„ç­–ç•¥ï¼šæ­¥éª¤è¡°å‡ï¼ˆStep Decayï¼‰å’ŒæŒç»­æ€§è¡°å‡ï¼ˆCosine Decayï¼‰ ^90b6c4

ä¸€ç§æ˜¯åœ¨å›ºå®šä½ç½®è¿›è¡Œè¡°å‡

ä¸€ç§æ˜¯é‡‡ç”¨å…¬å¼è¿›è¡Œè¡°å‡ï¼š

ä½™å¼¦å…¬å¼ï¼š
$$
\alpha_t = \frac{1}{\alpha_0}(1+cos(\frac{t\pi}{T}))
$$
$T$æ˜¯æ€»çš„epochæ•°

çº¿æ€§å…¬å¼ï¼š
$$
\alpha_t=\alpha_0(1-t/T)
$$
åå¹³æ–¹æ ¹è¡°å‡ï¼š
$$
\alpha_t = \alpha_0/(\sqrt t)
$$


### çº¿æ€§é¢„çƒ­ï¼ˆLinear Warmupï¼‰P94

çº¿æ€§é¢„çƒ­æ˜¯ä¸€ç§åœ¨è®­ç»ƒåˆæœŸé€æ­¥å¢åŠ å­¦ä¹ ç‡çš„æ–¹æ³•ï¼Œé˜²æ­¢è¿‡é«˜çš„åˆå§‹å­¦ä¹ ç‡å¯¼è‡´æŸå¤±å‡½æ•°çˆ†ç‚¸ã€‚å…·ä½“åœ°ï¼Œè¿™ç§æ–¹æ³•åœ¨è®­ç»ƒçš„å‰å¤§çº¦5000æ¬¡è¿­ä»£ä¸­ï¼Œçº¿æ€§åœ°ä»0å¢åŠ å­¦ä¹ ç‡ã€‚

### ä¸ºä»€ä¹ˆä½¿ç”¨çº¿æ€§é¢„çƒ­ï¼Ÿ

- é«˜åˆå§‹å­¦ä¹ ç‡å¯èƒ½ä¼šä½¿æŸå¤±å‡½æ•°å˜å¾—ä¸ç¨³å®šï¼Œç”šè‡³çˆ†ç‚¸ã€‚é€šè¿‡çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œå¯ä»¥é¿å…è¿™ç§æƒ…å†µå‘ç”Ÿï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¹³ç¨³åœ°è¿›å…¥è®­ç»ƒè¿‡ç¨‹ã€‚
- çº¿æ€§é¢„çƒ­æœ‰åŠ©äºæ¨¡å‹åœ¨åˆæœŸæ›´ç¨³å®šåœ°å­¦ä¹ ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆæœã€‚

### ç»éªŒæ³•åˆ™

å›¾ç‰‡ä¸­æåˆ°çš„ç»éªŒæ³•åˆ™æ˜¯ï¼š

- å¦‚æœå°†æ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰å¢åŠ  *N*å€ï¼Œé‚£ä¹ˆåˆå§‹å­¦ä¹ ç‡ä¹Ÿåº”å½“å¢åŠ  *N* å€ã€‚è¿™ç§åšæ³•å¯ä»¥ä½¿è®­ç»ƒè¿‡ç¨‹åœ¨ä¸åŒçš„æ‰¹é‡å¤§å°ä¸‹ä¿æŒç¨³å®šã€‚ ^28dd99

![image-20240716142955115](D:\zjPhD\notes\notes\AI\cs231n\å›¾ç‰‡\18.png)



![image-20240716142901588](D:\zjPhD\notes\notes\AI\cs231n\å›¾ç‰‡\17.png)

ä¸¤å¼ å›¾åˆ†åˆ«å¯¹æ¯”äº†ä¸åŒä¼˜åŒ–æ–¹æ³•çš„å·¥ä½œåŸç†å’ŒåŒºåˆ«

### ä¸€é˜¶ä¼˜åŒ–ï¼ˆFirst-Order Optimizationï¼‰

ä¸€é˜¶ä¼˜åŒ–ç®—æ³•åªä½¿ç”¨ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ä¿¡æ¯æ¥è¿›è¡Œä¼˜åŒ–ã€‚æœ€å¸¸è§çš„ä¸€é˜¶ä¼˜åŒ–ç®—æ³•æ˜¯æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰ã€‚

#### æ­¥éª¤

1. **ä½¿ç”¨æ¢¯åº¦å½¢æˆçº¿æ€§è¿‘ä¼¼**ï¼šè®¡ç®—å½“å‰ç‚¹çš„æ¢¯åº¦ï¼Œå¹¶ç”¨å®ƒæ¥çº¿æ€§è¿‘ä¼¼ç›®æ ‡å‡½æ•°ã€‚
2. **æ­¥è¿›ä»¥æœ€å°åŒ–è¿‘ä¼¼å€¼**ï¼šæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨ä¸€ä¸ªæ­¥é•¿ï¼Œä»¥å‡å°‘ç›®æ ‡å‡½æ•°å€¼ã€‚

#### å›¾ç¤º

- å›¾ä¸­å±•ç¤ºäº†æŸå¤±å‡½æ•°ï¼ˆLossï¼‰å…³äºæƒé‡ï¼ˆ$w_1$ï¼‰çš„æ›²çº¿ã€‚
- æ©™è‰²çº¿è¡¨ç¤ºåœ¨æŸä¸€ç‚¹çš„æ¢¯åº¦çº¿æ€§è¿‘ä¼¼ã€‚
- çº¢è‰²ç‚¹è¡¨ç¤ºä¼˜åŒ–è¿‡ç¨‹ä¸­æƒé‡çš„ä½ç½®ï¼Œé€šè¿‡æ²¿æ¢¯åº¦æ–¹å‘çš„åæ–¹å‘ç§»åŠ¨æ¥å‡å°‘æŸå¤±ã€‚

### äºŒé˜¶ä¼˜åŒ–ï¼ˆSecond-Order Optimizationï¼‰

äºŒé˜¶ä¼˜åŒ–ç®—æ³•ä¸ä»…ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯ï¼Œè¿˜ä½¿ç”¨<font color='orange'>HessiançŸ©é˜µ</font>ï¼ˆå³==ç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°==ï¼‰æ¥è¿›è¡Œä¼˜åŒ–ã€‚å¸¸è§çš„äºŒé˜¶ä¼˜åŒ–ç®—æ³•åŒ…æ‹¬ç‰›é¡¿æ³•ï¼ˆNewton's Methodï¼‰ã€‚

#### æ­¥éª¤

1. **ä½¿ç”¨æ¢¯åº¦å’ŒHessiançŸ©é˜µå½¢æˆäºŒæ¬¡è¿‘ä¼¼**ï¼šè®¡ç®—å½“å‰ç‚¹çš„æ¢¯åº¦å’ŒHessiançŸ©é˜µï¼Œå¹¶ç”¨å®ƒä»¬æ¥äºŒæ¬¡è¿‘ä¼¼ç›®æ ‡å‡½æ•°ã€‚
2. **æ­¥è¿›åˆ°è¿‘ä¼¼å€¼çš„æå°å€¼**ï¼šé€šè¿‡äºŒæ¬¡è¿‘ä¼¼çš„æå°å€¼æ¥æ›´æ–°æƒé‡ã€‚

#### å›¾ç¤º

- å›¾ä¸­å±•ç¤ºäº†æŸå¤±å‡½æ•°ï¼ˆLossï¼‰å…³äºæƒé‡ï¼ˆ$w_1$ï¼‰çš„æ›²çº¿ã€‚
- è“è‰²æ›²çº¿è¡¨ç¤ºåœ¨æŸä¸€ç‚¹çš„äºŒæ¬¡è¿‘ä¼¼ã€‚
- çº¢è‰²ç‚¹è¡¨ç¤ºä¼˜åŒ–è¿‡ç¨‹ä¸­æƒé‡çš„ä½ç½®ï¼Œé€šè¿‡æ²¿äºŒæ¬¡è¿‘ä¼¼çš„æå°å€¼æ–¹å‘ç§»åŠ¨æ¥å‡å°‘æŸå¤±ã€‚

ä½¿ç”¨äºŒé˜¶æ³°å‹’å±•å¼€ï¼š
$$
J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^\top \nabla_\theta J(\theta_0) + \frac{1}{2} (\theta - \theta_0)^\top H (\theta - \theta_0)
$$
$\theta_0$ æ˜¯å½“å‰å‚æ•°ï¼›$\nabla_\theta J(\theta_0)$ æ˜¯åœ¨ $\theta_0$ å¤„çš„æ¢¯åº¦ ï¼›$H$ æ˜¯åœ¨ $\theta_0$ å¤„çš„ Hessian çŸ©é˜µï¼ˆç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°çŸ©é˜µï¼‰

é€šè¿‡æ±‚è§£ä¸´ç•Œç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ç‰›é¡¿å‚æ•°æ›´æ–°å…¬å¼ï¼š:
$$
\theta^\star = \theta_0=H^{-1}\mathbf\nabla_{\theta}J(\theta_0)
$$
å¦‚ä½•å¾—æ±‚è§£ä¸´ç•Œç‚¹ï¼Ÿå…·ä½“è®¡ç®—å¦‚ä¸‹ï¼š



å¯¹$\theta$æ±‚å¯¼
$$
\nabla_\theta J(\theta) \approx \nabla_\theta J(\theta_0) + \nabla_\theta \left( (\theta - \theta_0)^\top \nabla_\theta J(\theta_0) \right) + \nabla_\theta \left( \frac{1}{2} (\theta - \theta_0)^\top H (\theta - \theta_0) \right)
$$

* å¸¸æ•°é¡¹ $J(\theta_0)$ çš„å¯¼æ•°ä¸ºé›¶

* $(\theta - \theta_0)^\top \nabla_\theta J(\theta_0)$ çš„å¯¼æ•°ï¼š$$\nabla_\theta \left( (\theta - \theta_0)^\top \nabla_\theta J(\theta_0) \right) = \nabla_\theta J(\theta_0).$$

* $\frac{1}{2} (\theta - \theta_0)^\top H (\theta - \theta_0)$ çš„å¯¼æ•°ï¼š$\nabla_\theta \left( \frac{1}{2} (\theta - \theta_0)^\top H (\theta - \theta_0) \right) = H (\theta - \theta_0)$

  è¿™ä¸€éƒ¨åˆ†è®¾è®¡<font color='orange'>çŸ©é˜µæ±‚å¯¼</font>çš„ç»“æœï¼Œå¯¹äºå¯¹ç§°çš„HessiançŸ©é˜µ.

æ±‚è§£$\theta $çš„æ›´æ–°å…¬å¼
$$
H(\theta-\theta_0) = -\mathbf \nabla_{\theta}J(\theta_0)\\
\theta-\theta_0 = -H^{-1}\mathbf \nabla_\theta J(\theta_0)\\
\theta = \theta_0 - H^{-1}\mathbf \nabla_\theta J(\theta_0)
$$
<font color='red'>ä¸ºä»€ä¹ˆè¿™å¯¹æ·±åº¦å­¦ä¹ ä¸åˆ©ï¼Ÿ</font>

* Hessianæœ‰ $O(N^2)$ ä¸ªå…ƒç´ 
* è®¡ç®—é€†çŸ©é˜µéœ€è¦ $O(N^3)$ çš„æ—¶é—´å¤æ‚åº¦
* $N = \text{ï¼ˆæ•°åƒä¸‡åˆ°æ•°äº¿ï¼‰}$



### 1. Adam(W) æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é€‰æ‹©

- Adam(W)

  ï¼šAdamä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬å¸¦æƒé‡è¡°å‡çš„AdamWï¼‰åœ¨è®¸å¤šæƒ…å†µä¸‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤é€‰æ‹©ã€‚

  - **ä¼˜åŠ¿**ï¼šAdamèƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ï¼Œå¹¶ä¸”åœ¨å¤„ç†ç¨€ç–æ¢¯åº¦å’Œå™ªå£°è¾ƒå¤§çš„æ•°æ®æ—¶è¡¨ç°è‰¯å¥½ã€‚
  - **å›ºå®šå­¦ä¹ ç‡**ï¼šå³ä½¿ä½¿ç”¨å›ºå®šçš„å­¦ä¹ ç‡ï¼ŒAdamé€šå¸¸ä¹Ÿèƒ½å–å¾—ä¸é”™çš„æ•ˆæœï¼Œå› ä¸ºå®ƒå†…éƒ¨çš„è‡ªé€‚åº”å­¦ä¹ ç‡æœºåˆ¶èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹æ›´ç¨³å®šåœ°æ”¶æ•›ã€‚

### 2. SGD + åŠ¨é‡

- SGD + åŠ¨é‡ï¼ˆMomentumï¼‰

  ï¼šå¸¦æœ‰åŠ¨é‡çš„éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGD with Momentumï¼‰åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥è¶…è¿‡Adamçš„è¡¨ç°ã€‚

  - **ä¼˜åŠ¿**ï¼šåŠ¨é‡å¯ä»¥åŠ é€ŸSGDåœ¨å‡¹é¢åŒºåŸŸçš„æ”¶æ•›ï¼Œå¹¶ä¸”åœ¨ä¸€å®šç¨‹åº¦ä¸Šå…‹æœäº†SGDçš„æŒ¯è¡é—®é¢˜ã€‚
  - **è°ƒå‚éœ€æ±‚**ï¼šä½¿ç”¨SGD + åŠ¨é‡æ—¶ï¼Œé€šå¸¸éœ€è¦æ›´å¤šçš„è¶…å‚æ•°è°ƒä¼˜ï¼Œä¾‹å¦‚å­¦ä¹ ç‡å’Œå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚

### 3. å¦‚æœå¯ä»¥è¿›è¡Œå…¨æ‰¹é‡æ›´æ–°

- å…¨æ‰¹é‡æ›´æ–°

  ï¼šå¦‚æœæœ‰èƒ½åŠ›è¿›è¡Œå…¨æ‰¹é‡æ›´æ–°ï¼Œé‚£ä¹ˆå¯ä»¥è€ƒè™‘è¶…è¶Šä¸€é˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿ç”¨äºŒé˜¶ä¼˜åŒ–æ–¹æ³•æˆ–æ›´é«˜çº§çš„æ–¹æ³•ã€‚

  - **äºŒé˜¶ä¼˜åŒ–ï¼ˆSecond-Order Optimizationï¼‰**ï¼šäºŒé˜¶ä¼˜åŒ–æ–¹æ³•åˆ©ç”¨ç›®æ ‡å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ä¿¡æ¯ï¼ˆHessiançŸ©é˜µï¼‰æ¥æ›´æ–°å‚æ•°ï¼Œç†è®ºä¸Šæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œä½†è®¡ç®—æˆæœ¬å’Œå†…å­˜éœ€æ±‚è¾ƒé«˜ã€‚
  - **é€‚ç”¨æ€§**ï¼šç”±äºäºŒé˜¶ä¼˜åŒ–æ–¹æ³•çš„è®¡ç®—å¤æ‚åº¦ï¼Œé€šå¸¸åªåœ¨èƒ½å¤Ÿè¿›è¡Œå…¨æ‰¹é‡æ›´æ–°ï¼ˆbatch sizeéå¸¸å¤§ï¼‰æ—¶è€ƒè™‘ä½¿ç”¨ã€‚

æˆ‘ä»¬ä¸Šé¢çš„ä¼˜åŒ–çš„æ˜¯æ¯”è¾ƒç®€å•çš„æƒ…å†µï¼Œé‚£ä¹ˆå¦‚æœæˆ‘ä»¬æƒ³è¦ä¼˜åŒ–æ›´åŠ å¤æ‚çš„å‡½æ•°å‘¢ï¼Ÿ



å½“å‰ï¼Œçº¿æ€§æŸå¤±å‡½æ•°$f=Wx$

ä¸‹æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶2å±‚çš„ç¥ç»ç½‘ç»œ$f=W_2max(0,W_1x)$

where $x\in \mathbb{R}^D,W_1\in\mathbb{R}^{H\times D},W_2 \in \mathbb{R}^{C \times H}$



äºŒé˜¶ä¼˜åŒ–ä¸­çš„ç‰›é¡¿æ³•ï¼Œè®¡ç®—é‡è¾ƒå¤§ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨æ‹Ÿç‰›é¡¿æ³•ï¼š

BFGS æ–¹æ³•ï¼š

â€‹	BFGSï¼ˆBroydenâ€“Fletcherâ€“Goldfarbâ€“Shannoï¼‰æ–¹æ³•æ˜¯æœ€å¸¸ç”¨çš„æ‹Ÿç‰›é¡¿æ–¹æ³•ä¹‹ä¸€ã€‚

- **ä¸ç›´æ¥æ±‚é€†**ï¼šBFGSæ–¹æ³•ä¸ç›´æ¥è®¡ç®—HessiançŸ©é˜µçš„é€†ï¼ˆè¿™éœ€è¦ ğ‘‚(ğ‘3)*O*(*N*3) çš„è®¡ç®—é‡ï¼‰ï¼Œè€Œæ˜¯é€šè¿‡é€æ­¥æ›´æ–°ä¸€ä¸ªè¿‘ä¼¼çš„é€†HessiançŸ©é˜µã€‚
- **ç§©1æ›´æ–°**ï¼šBFGSé€šè¿‡é€æ­¥è¿›è¡Œç§©1æ›´æ–°æ¥è¿‘ä¼¼é€†HessiançŸ©é˜µï¼Œæ¯æ¬¡æ›´æ–°çš„è®¡ç®—é‡æ˜¯ ğ‘‚(ğ‘2)*O*(*N*2)ï¼Œè¿™æ¯”ç›´æ¥æ±‚é€†è¦é«˜æ•ˆå¾—å¤šã€‚



L-BFGSæ–¹æ³•ï¼šæœ‰é™å†…å­˜BFGSæ–¹æ³•ï¼š

â€‹	L-BFGSï¼ˆLimited-memory BFGSï¼‰æ–¹æ³•æ˜¯BFGSæ–¹æ³•çš„å˜ç§ï¼Œä¸“é—¨ç”¨äºé«˜ç»´é—®é¢˜ï¼Œè§£å†³äº†å­˜å‚¨å’Œè®¡ç®—çš„é—®é¢˜ã€‚

- **æœ‰é™å†…å­˜**ï¼šL-BFGSä¸å½¢æˆæˆ–å­˜å‚¨å®Œæ•´çš„é€†HessiançŸ©é˜µï¼Œè€Œæ˜¯åªå­˜å‚¨å’Œæ›´æ–°å°‘é‡çš„å†å²ä¿¡æ¯ï¼ˆå¦‚æ¢¯åº¦å’Œå‚æ•°çš„å˜åŒ–ï¼‰ã€‚
- **é€‚ç”¨æ€§**ï¼šç”±äºå…¶æœ‰é™å†…å­˜çš„ç‰¹ç‚¹ï¼ŒL-BFGSéå¸¸é€‚ç”¨äºé«˜ç»´åº¦ã€å¤§è§„æ¨¡ä¼˜åŒ–é—®é¢˜ï¼Œæ¯”å¦‚æ·±åº¦å­¦ä¹ ä¸­çš„ä¼˜åŒ–é—®é¢˜ã€‚
- é€šå¸¸åœ¨å…¨æ‰¹é‡ï¼ˆFull Batchï¼‰ã€ç¡®å®šæ€§æ¨¡å¼ä¸‹è¡¨ç°éå¸¸å¥½ï¼Œå¦‚æœæœ‰ä¸€ä¸ªå•ä¸€çš„ç¡®å®šæ€§çš„ç›®æ ‡å‡½æ•°$f(x)$ï¼Œé‚£ä¹ˆæ•ˆæœä¼šéå¸¸å¥½

> ### å…³é”®ç‚¹æ€»ç»“
>
> - **æ‹Ÿç‰›é¡¿æ³•ï¼ˆBFGSï¼‰**ï¼šé€šè¿‡é€æ­¥è¿›è¡Œç§©1æ›´æ–°æ¥è¿‘ä¼¼é€†HessiançŸ©é˜µï¼Œè®¡ç®—å¤æ‚åº¦ä¸º $O(N^2)$ï¼Œè€Œä¸æ˜¯ç›´æ¥æ±‚é€†çš„ $O(N^3)$ã€‚
> - **æœ‰é™å†…å­˜BFGSï¼ˆL-BFGSï¼‰**ï¼šä¸å½¢æˆæˆ–å­˜å‚¨å®Œæ•´çš„é€†HessiançŸ©é˜µï¼Œåªå­˜å‚¨æœ‰é™çš„å†å²ä¿¡æ¯ï¼Œéå¸¸é€‚åˆé«˜ç»´ä¼˜åŒ–é—®é¢˜ã€‚

$$
\mathcal{F}(x; \mathbf{w}) \in \mathbb{R}^n
$$

