<center>lecture13 生成模型</center>

# 分类

## 显式密度估计

直接求解概率分布$p_{model}(x)$建模，并且直接想要求解分布的具体形式

## 隐式密度估计

不直接求解概率分布，而是训练一个模型，从分布中采样。

> 思考一个例子，隐式密度估计更多的是比如有一堆人脸图片，我希望训练一个模型，==生成新的，逼真的人脸图片==。

![image-20240819131545782](D:\zjPhD\notes\notes\AI\cs231n\图片\\44.png)

# PixelRNN and PixelCNN 

PixelRNN and PixelCNN 的生成对像素具有一定的依赖性，每个像素的生成依赖于之前生成的像素
$$
p_{\theta}(x) = \prod_{i=1}^n p_{\theta}(x_i|x_1…,x_{i-1})
$$

# 变分自编码器（VAE）

定义了一个不可解密度函数（intractable density function），这个过程引入了潜在变量$z$
$$
p_{\theta}(x) = \int p_{\theta}(z)p_{\theta}(x|z)dz
$$
$p_{\theta}(x) $是一个不可解密的密度函数

VAE：==像素之间没有直接的依赖性，所有像素可以同时生成==

**优化方法 (Optimization)**:

- **PixelRNN/CNNs**: 可以直接优化训练数据的似然，因为其定义了可解的密度函数。
- **VAEs**: 由于密度函数不可解，无法直接优化似然，而是推导出一个似然的下界，并对这个下界进行优化。

**潜在变量 $z$ 的作用 (Why latent $z$?)**:

- 在 VAEs 中，引入了潜在变量 zzz，它捕捉了数据的潜在结构或特征。通过对潜在空间的建模，VAEs 可以生成具有特定特征或风格的样本，这为生成模型提供了更大的灵活性和表达能力。

# 自编码

## 介绍

**自编码器（Autoencoder）**的核心任务是学习一个能够重建原始数据的低维特征表示。

1. 输入数据（x）:
   输入的是高维数据，比如图像
2. 编码器（Encoder）：
   编码器将输入数据压缩到一个低维的潜在空间表示$z$，这通常被成为特征
   这个压缩过程通常伴随着降维，即潜在空间的维度$z$通常比输入数据的维度$x$更小
3. 解码器（Decoder）:
   从低维特征$z$重建出与原始数据$x$尽可能接近的输出
   目标是使得$\hat x$和$x$尽量相似

## 应用

使用L2损失函数来衡量原始数据和重建数据之间的差异

不使用标签

***应用2：***

训练完成后，可以丢弃解码器，==只保留编码器==，这种方式可以用于特征提取，提取的特征可以用于后续的任务，如分类或聚类

**应用2：**

先使用大规模无标签数据寻来你自编码器，然后将编码器迁移到带有标签的小数据集上，进行监督学习

![image-20240819150044511](D:\zjPhD\notes\notes\AI\cs231n\图片\45.png)

## 变分自编码器（VAE）的不可解性问题详解

![image-20240819154149598](D:\zjPhD\notes\notes\AI\cs231n\图片\46.png)

